<header id="articleheader">
  <h2 id="articletitle">What are HashTables?</h2>
  <time id="articledate">Published July 1, 2020</time>
</header>
<article id="articlecontent">
  <p>If you're unfamiliar with hashtables, then you've likely only used standard arrays and lists to store data so far. With those simpler data structures, operations such as search and deletion take linear time, <var>O(n)</var>, or in certain implementations, logarithmic time, <var>O(log n)</var>. For hashtables, however, these operations can be done in amortized constant time, <var>O(1)</var>. Loosely speaking, using hashtables allow you to access data instantly.</p>
  <img src="https://i.imgur.com/Y3p19VO.png" alt="Hashtables shown as the Flash superhero." title="I'm Hash Table, and I'm the fastest data structure alive." />
  <p class="disclaimer"><strong>DISCLAIMER:&nbsp;</strong><em>I am by no means an expert in programming, so keep in mind that it is possible that I may be wrong here and there. At the time of writing this post, I have assisted in teaching data structures at my university for three semesters, and have taught hashtables multiple times. While I have a fairly solid understanding of hashtables, I have also learned something new each semester I have taught, so it is possible that I have misconceptions. If you would like some help in finding educational resources, check out the /r/learnprogramming subreddit wiki <a href="https://www.reddit.com/r/learnprogramming/wiki/faq" target="_blank">[here]</a>. While I still have your attention, [<a href="https://www.reddit.com/r/learnprogramming/wiki/index#wiki_discouraged_resources" target="_blank">here</a>] is a list of resources discouraged by /r/learnprogramming either due to unreliable or misleading information.</em></p>
<br />
<h2>Introduction to Time Complexity</h2>
<p>Before I launch into an explanation of hash tables, let's discuss time complexity of operations for other data structures first. Time complexity itself can be a whole other post, so I'll keep it simple for those of you unfamiliar with the term. In short, time complexity describes the amount of time taken by an algorithm as the input size grows. Note that it does not specify how much time the algorithm <b>will</b> take, it simply describes how the amount of time taken will grow as input grows. In the case of big-O notation, it describes an upper bound for the growth function of time for the algorithm, and generally refers to the worst-case.<span class="tooltip"><span class="tooltipnumber">[1]</span><span class="tooltiptext">There's more nuances to time complexity than just this, but I'm not going to go into that here.</span></span> For example, if we have an algorithm with an <var>O(n)</var> time complexity, where n represents the input size, then if we increase <var>n</var>, we should see the amount of time taken grow linearly with respect to <var>n</var>. For an algorithm with <var>O(n<sup>2</sup>)</var> time complexity, as <var>n</var> increases, the amount of time increases quadratically.</p>
<figure>
  <img src="https://i.imgur.com/kAv9QmZ.jpg" alt="Graph showing time complexity for O(n^2), O(n), O(log n)." />
  <br />
  <figcaption>I specifically avoided adding numbers to the graph because the numbers don't actually matter. The shape of the graphed function is what is important, and we can see the pattern of how time grows as input size grows.</figcaption>
</figure>
<p>With time complexity out of the way, let's talk about the big-O time complexity for some of the other data structures. The first data structure you usually learn is the array, which is essentially just an ordered list of items. For a standard unsorted array, if you want to determine if item X exists in the array, you simply have to check every item until you find X, or until you reach the end.</p>
</article>
